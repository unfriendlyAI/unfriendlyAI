{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Qlearning-tensorflow 2.0",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygvryHT5PUGi",
        "colab_type": "text"
      },
      "source": [
        "# Реализация Q-learning в Tensorflow 2.0\n",
        "\n",
        "Задачу Q-learning из Coursera ([Practical Reinforcement Learning Неделя 4 Approximate Q-learning: CartPole](https://www.coursera.org/learn/practical-rl/notebook/pO7NP/approximate-q-learning-cartpole)) , в которой в шаблоне использовались элементы Tensorflow 1.0 (от одного вида placeholder и sess.run уже не хочется учиться) перевожу в простой и читаемый Tensorflow 2.0 (Keras)   вид. \n",
        "Получилось не только упростить агента, но и разобраться в особенности Q-learning\n",
        "\n",
        "Основная проблема, которая возникла в моей версии реализации - что нейросеть получает на вход вектор состояния, а на выходе дает вектор с ценностью каждого действия для этого состояния. В то же время обучающий пример дает нам уточнение ценности только одного действия для исходного состояния. Остальные действия (значение остальных выходных нейронов) нам неизвестны. Как же применить обучение с учителем (tf.keras.model.fit) если из вектора Y известно \"истинное\" значение только для одного из нейронов?\n",
        "\n",
        "Решение следелал следующее. \n",
        "1. Определяю предсказание нейросети Yp (все действия) для исходного состояния s (predict(s))\n",
        "2. Вычисляю уточненную цель Qsa для предпринятого действия (одного)\n",
        "3. В векторе Yp заменяю значение для предпринятого действия на Qsa\n",
        "4. Запускаю fit(s,Yp) для исходного состояния s и подправленного вектора Yp. \n",
        "\n",
        "В результате оптимизатор как раз и сделает один шаг в уменьшении MSE между предсказанным и вычисленным значениями Qsa действия из примера.\n",
        "\n",
        "Остальные параметры модели сохранил из решения исходной задачи (чтобы возможно было ставнить работоспособность). Текст исходной задачи (комментарии на английском) тоже сохранил (убрав под CENSORED ненужные части реализации в Tensorflow 1.0). Добавленные мной строки комментировал на русском.\n",
        "\n",
        "Вывод - результаты работы нейросети в TF 1.0 и TF 2.0 совпали. Программа стала более чем на десяток строк короче."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efbQyd98I7r1",
        "colab_type": "text"
      },
      "source": [
        "# Approximate q-learning\n",
        "\n",
        "In this notebook you will teach a __tensorflow__ neural network to do Q-learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN_FhBt2I7r4",
        "colab_type": "text"
      },
      "source": [
        "__Frameworks__ - we'll accept this homework in any deep learning framework. This particular notebook was designed for tensorflow, but you will find it easy to adapt it to almost any python-based deep learning framework."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d42-c6XeI7r6",
        "colab_type": "code",
        "outputId": "b78b5f4f-c591-4004-d16c-91c691139577",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules:\n",
        "   \n",
        "    if not os.path.exists('.setup_complete'):\n",
        "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n",
        "\n",
        "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
        "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week4_approx/submit.py\n",
        "\n",
        "        !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 144467 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.4_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.4) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTRPwrWnI7sH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZk3m6MKI7sQ",
        "colab_type": "code",
        "outputId": "78c1314c-d531-49e2-9af6-19ae8313d157",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "env = gym.make(\"CartPole-v0\").env\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fdadbffd588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAATkElEQVR4nO3df6zddZ3n8eert+WHgFOw19JpC2WG7iozWQu5ixg1i4gjspOBSVwDuyIxJJ1NMNHEzC7MJjuaLMlMXMQx65JlAiuuroijSCXMOoAks2ZXoGgthYJWLdJuSwuUAmILbd/7x/0WD7Sl5/7i9HPP85Gc3O/3/f18z3l/4uHlt5/7PfekqpAktWPOoBuQJE2MwS1JjTG4JakxBrckNcbglqTGGNyS1JgZC+4kFyR5LMmGJFfN1OtI0rDJTNzHnWQE+CnwAWAT8ABwaVU9Mu0vJklDZqauuM8GNlTVL6rqJeAW4KIZei1JGipzZ+h5FwNP9OxvAt55qMELFiyoZcuWzVArktSejRs38tRTT+Vgx2YquA8ryUpgJcApp5zC6tWrB9WKJB1xxsbGDnlsppZKNgNLe/aXdLVXVNUNVTVWVWOjo6Mz1IYkzT4zFdwPAMuTnJbkKOASYNUMvZYkDZUZWSqpqj1JPgF8DxgBbqqqh2fitSRp2MzYGndV3QncOVPPL0nDyk9OSlJjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqzJS+uizJRuB5YC+wp6rGkpwEfANYBmwEPlJVO6bWpiRpv+m44n5fVa2oqrFu/yrgnqpaDtzT7UuSpslMLJVcBNzcbd8MXDwDryFJQ2uqwV3APyR5MMnKrrawqrZ021uBhVN8DUlSjymtcQPvqarNSd4K3JXk0d6DVVVJ6mAndkG/EuCUU06ZYhuSNDymdMVdVZu7n9uA24CzgSeTLALofm47xLk3VNVYVY2Njo5OpQ1JGiqTDu4kxyU5Yf828EfAOmAVcHk37HLg9qk2KUn6rakslSwEbkuy/3n+Z1X9ryQPALcmuQJ4HPjI1NuUJO036eCuql8A7zhI/Wng/VNpSpJ0aH5yUpIaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWrMYYM7yU1JtiVZ11M7KcldSX7W/TyxqyfJF5NsSLI2yVkz2bwkDaN+rri/DFzwmtpVwD1VtRy4p9sH+BCwvHusBK6fnjYlSfsdNrir6h+BZ15Tvgi4udu+Gbi4p/6VGvdDYH6SRdPVrCRp8mvcC6tqS7e9FVjYbS8GnugZt6mrHSDJyiSrk6zevn37JNuQpOEz5V9OVlUBNYnzbqiqsaoaGx0dnWobkjQ0JhvcT+5fAul+buvqm4GlPeOWdDVJ0jSZbHCvAi7vti8Hbu+pf6y7u+QcYGfPkookaRrMPdyAJF8HzgUWJNkE/CXwV8CtSa4AHgc+0g2/E7gQ2AC8CHx8BnqWpKF22OCuqksPcej9BxlbwJVTbUqSdGh+clKSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMOG9xJbkqyLcm6ntpnkmxOsqZ7XNhz7OokG5I8luSDM9W4JA2rfq64vwxccJD6dVW1onvcCZDkDOAS4A+6c/5rkpHpalaS1EdwV9U/As/0+XwXAbdU1e6q+iXj3/Z+9hT6kyS9xlTWuD+RZG23lHJiV1sMPNEzZlNXO0CSlUlWJ1m9ffv2KbQhScNlssF9PfD7wApgC3DtRJ+gqm6oqrGqGhsdHZ1kG5I0fCYV3FX1ZFXtrap9wN/y2+WQzcDSnqFLupokaZpMKriTLOrZ/VNg/x0nq4BLkhyd5DRgOXD/1FqUJPWae7gBSb4OnAssSLIJ+Evg3CQrgAI2An8GUFUPJ7kVeATYA1xZVXtnpnVJGk6HDe6quvQg5RtfZ/w1wDVTaUqSdGh+clKSGmNwS1JjDG5JaozBLUmNMbglqTEGt4bWS79+luc2P8qe3S8OuhVpQg57O6A0W/xmxxY2/d9bX9l/6dc72LVjC//0T/6c408+fYCdSRNjcGto7H3pRZ7b9Mig25CmzKUSSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW0MjmUPmjBxQ3/mrh6iqAXQkTY7BraHxpgWncMLitx9Qf37LTwfQjTR5BreGRuaMMGfEz5ypfQa3JDXmsMGdZGmSe5M8kuThJJ/s6icluSvJz7qfJ3b1JPlikg1J1iY5a6YnIUnDpJ8r7j3Ap6vqDOAc4MokZwBXAfdU1XLgnm4f4EOMf7v7cmAlcP20dy1JQ+ywwV1VW6rqR93288B6YDFwEXBzN+xm4OJu+yLgKzXuh8D8JIumvXNJGlITWuNOsgw4E7gPWFhVW7pDW4GF3fZi4Ime0zZ1tdc+18okq5Os3r59+wTblqTh1XdwJzke+Bbwqap6rvdYjd8EO6EbYavqhqoaq6qx0dHRiZwqSUOtr+BOMo/x0P5aVX27Kz+5fwmk+7mtq28GlvacvqSrSZKmQT93lQS4EVhfVZ/vObQKuLzbvhy4vaf+se7uknOAnT1LKpKkKern0wjvBi4DHkqypqv9BfBXwK1JrgAeBz7SHbsTuBDYALwIfHxaO5akIXfY4K6qHwA5xOH3H2R8AVdOsS9pRmRk3gG12reP2reX+KlKNcJPTmqonLzig5BXX4e8+NSveH7z+gF1JE2cwa2hMucgV9zU+BW31AqDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG4NlZGjj+PYk5YcUH/+//2U8b+PJh35DG4NlXnHnsBxb112QH3nE+vA4FYjDG5JaozBLUmNMbglqTEGtyQ1pp8vC16a5N4kjyR5OMknu/pnkmxOsqZ7XNhzztVJNiR5LMkHZ3ICkjRs+vmSvT3Ap6vqR0lOAB5Mcld37Lqq+s+9g5OcAVwC/AHwu8DdSf5JVfkVI5I0DQ57xV1VW6rqR93288B6YPHrnHIRcEtV7a6qXzL+be9nT0ezkqQJrnEnWQacCdzXlT6RZG2Sm5Kc2NUWA0/0nLaJ1w96SdIE9B3cSY4HvgV8qqqeA64Hfh9YAWwBrp3ICydZmWR1ktXbt2+fyKmSNNT6Cu4k8xgP7a9V1bcBqurJqtpbVfuAv+W3yyGbgaU9py/paq9SVTdU1VhVjY2Ojk5lDpI0VPq5qyTAjcD6qvp8T31Rz7A/BdZ126uAS5IcneQ0YDlw//S1LE1N5owcUNu7+9f85plNA+hGmrh+7ip5N3AZ8FCSNV3tL4BLk6wACtgI/BlAVT2c5FbgEcbvSLnSO0p0JHnrH57H04/9H/bteemV2p5dL/Dr7Y/zpgWnDLAzqT+HDe6q+gGQgxy683XOuQa4Zgp9STNmZN6xg25BmhI/OSlJjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuDV8cvC/V1L79lJVA2hImhiDW0Nn7jHHs+Bt7z2gvu2he6i9Lw+gI2liDG4NnWQOc+YdfUB978u7vOJWEwxuSWpMP3/WVWrC3XffzZe+9KW+xr739OP4F8uPe1Xt2Wd3cOmll/Ly3sNfdS9dupQvfOELzJnjtY/eeAa3Zo3HH3+c73znO32NHf2XZ/Ge09/Jnn1HAZDsY9euHXz3u99l10t7Dnv+29/+dpdVNDAGt4bSPubw0M73smXXaQDMy25OnbNqwF1J/fHfeRpKO19ewNZdy9hb89hb89i173jW7Hwfe8trGR35DG4Npe27l7KnjnpVbc++eQPqRpqYfr4s+Jgk9yf5SZKHk3y2q5+W5L4kG5J8I8lRXf3obn9Dd3zZzE5BmrjfPXYD87LrVbVjR14guG6tI18/V9y7gfOq6h3ACuCCJOcAfw1cV1WnAzuAK7rxVwA7uvp13TjpiHLcyHOcdtw6jht5lud3PsFzz6xn9OXbCIf/xaQ0aP18WXABL3S787pHAecB/7qr3wx8BrgeuKjbBvg74L8kSfkreB1BfvDQ4zz93N9QhP+99nGeee43hGKfb1M1oK/fxCQZAR4ETge+BPwceLaq9l+ebAIWd9uLgScAqmpPkp3AW4CnDvX8W7du5XOf+9ykJiDt98ADD/Q99tFfPcWjv3r1W3Iikf30009z7bXXkmQCZ0n927p16yGP9RXcVbUXWJFkPnAb8LapNpVkJbASYPHixVx22WVTfUoNublz5/LNb37zDXmt+fPn89GPftQP4GjGfPWrXz3ksQnd+1RVzya5F3gXMD/J3O6qewmwuRu2GVgKbEoyF/gd4OmDPNcNwA0AY2NjdfLJJ0+kFekAb37zm9+w1xoZGWHhwoWMjBz4Vwal6TBv3qHvcurnrpLR7kqbJMcCHwDWA/cCH+6GXQ7c3m2v6vbpjn/f9W1Jmj79XHEvAm7u1rnnALdW1R1JHgFuSfKfgB8DN3bjbwT+R5INwDPAJTPQtyQNrX7uKlkLnHmQ+i+Asw9S3wX8q2npTpJ0AH+zIkmNMbglqTH+RR3NGqeeeioXX3zxG/JaS5cu9R5uDYzBrVnj/PPP5/zzzx90G9KMc6lEkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDWmny8LPibJ/Ul+kuThJJ/t6l9O8sska7rHiq6eJF9MsiHJ2iRnzfQkJGmY9PP3uHcD51XVC0nmAT9I8vfdsT+vqr97zfgPAcu7xzuB67ufkqRpcNgr7hr3Qrc7r3vU65xyEfCV7rwfAvOTLJp6q5Ik6HONO8lIkjXANuCuqrqvO3RNtxxyXZKju9pi4Ime0zd1NUnSNOgruKtqb1WtAJYAZyf5Q+Bq4G3APwdOAv79RF44ycokq5Os3r59+wTblqThNaG7SqrqWeBe4IKq2tIth+wG/jtwdjdsM7C057QlXe21z3VDVY1V1djo6OjkupekIdTPXSWjSeZ328cCHwAe3b9unfGvur4YWNedsgr4WHd3yTnAzqraMiPdS9IQ6ueukkXAzUlGGA/6W6vqjiTfTzIKBFgD/Ntu/J3AhcAG4EXg49PftiQNr8MGd1WtBc48SP28Q4wv4MqptyZJOhg/OSlJjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhqTqhp0DyR5Hnhs0H3MkAXAU4NuYgbM1nnB7J2b82rLqVU1erADc9/oTg7hsaoaG3QTMyHJ6tk4t9k6L5i9c3Nes4dLJZLUGINbkhpzpAT3DYNuYAbN1rnN1nnB7J2b85oljohfTkqS+nekXHFLkvo08OBOckGSx5JsSHLVoPuZqCQ3JdmWZF1P7aQkdyX5WffzxK6eJF/s5ro2yVmD6/z1JVma5N4kjyR5OMknu3rTc0tyTJL7k/ykm9dnu/ppSe7r+v9GkqO6+tHd/obu+LJB9n84SUaS/DjJHd3+bJnXxiQPJVmTZHVXa/q9OBUDDe4kI8CXgA8BZwCXJjljkD1NwpeBC15Tuwq4p6qWA/d0+zA+z+XdYyVw/RvU42TsAT5dVWcA5wBXdv/btD633cB5VfUOYAVwQZJzgL8Grquq04EdwBXd+CuAHV39um7ckeyTwPqe/dkyL4D3VdWKnlv/Wn8vTl5VDewBvAv4Xs/+1cDVg+xpkvNYBqzr2X8MWNRtL2L8PnWA/wZcerBxR/oDuB34wGyaG/Am4EfAOxn/AMfcrv7K+xL4HvCubntuNy6D7v0Q81nCeICdB9wBZDbMq+txI7DgNbVZ816c6GPQSyWLgSd69jd1tdYtrKot3fZWYGG33eR8u39GnwncxyyYW7ecsAbYBtwF/Bx4tqr2dEN6e39lXt3xncBb3tiO+/YF4N8B+7r9tzA75gVQwD8keTDJyq7W/Htxso6UT07OWlVVSZq9dSfJ8cC3gE9V1XNJXjnW6tyqai+wIsl84DbgbQNuacqS/DGwraoeTHLuoPuZAe+pqs1J3grcleTR3oOtvhcna9BX3JuBpT37S7pa655Msgig+7mtqzc13yTzGA/tr1XVt7vyrJgbQFU9C9zL+BLC/CT7L2R6e39lXt3x3wGefoNb7ce7gT9JshG4hfHlkr+h/XkBUFWbu5/bGP8/27OZRe/FiRp0cD8ALO9+830UcAmwasA9TYdVwOXd9uWMrw/vr3+s+633OcDOnn/qHVEyfml9I7C+qj7fc6jpuSUZ7a60SXIs4+v26xkP8A93w147r/3z/TDw/eoWTo8kVXV1VS2pqmWM/3f0/ar6NzQ+L4AkxyU5Yf828EfAOhp/L07JoBfZgQuBnzK+zvgfBt3PJPr/OrAFeJnxtbQrGF8rvAf4GXA3cFI3NozfRfNz4CFgbND9v8683sP4uuJaYE33uLD1uQH/DPhxN691wH/s6r8H3A9sAL4JHN3Vj+n2N3THf2/Qc+hjjucCd8yWeXVz+En3eHh/TrT+XpzKw09OSlJjBr1UIkmaIINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTG/H9lBasz3JPIrgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttFisl98I7sZ",
        "colab_type": "text"
      },
      "source": [
        "# Approximate (deep) Q-learning: building the network\n",
        "\n",
        "To train a neural network policy one must have a neural network policy. Let's build it.\n",
        "\n",
        "\n",
        "Since we're working with a pre-extracted features (cart positions, angles and velocities), we don't need a complicated network yet. In fact, let's build something like this for starters:\n",
        "\n",
        "![img](https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/yet_another_week/_resource/qlearning_scheme.png)\n",
        "\n",
        "For your first run, please only use linear layers (`L.Dense`) and activations. Stuff like batch normalization or dropout may ruin everything if used haphazardly. \n",
        "\n",
        "Also please avoid using nonlinearities like sigmoid & tanh: since agent's observations are not normalized, sigmoids might be saturated at initialization. Instead, use non-saturating nonlinearities like ReLU.\n",
        "\n",
        "Ideally you should start small with maybe 1-2 hidden layers with < 200 neurons and then increase network size if agent doesn't beat the target score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gI_L8REiI7sa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dab425e0-9938-4a80-f824-321592a2db72"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import keras.layers as L\n",
        "\n",
        "#   CENSORED!!!!\n",
        "# tf.reset_default_graph()\n",
        "# sess = tf.InteractiveSession()\n",
        "# keras.backend.set_session(sess)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRIvP5WBI7sk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network = keras.models.Sequential()\n",
        "network.add(L.InputLayer(state_dim))\n",
        "network.add(L.Dense(64,activation=\"relu\"))\n",
        "network.add(L.Dense(128,activation=\"relu\"))\n",
        "network.add(L.Dense(64,activation=\"relu\"))\n",
        "network.add(L.Dense(n_actions))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(lr=1e-4)   # строка добавлена как в шаблоне, чтобы проверить совпадение результатов\n",
        "\n",
        "network.compile(loss='mse', optimizer=optimizer,  metrics=['mse'])  # строка добавлена\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r86eH1dHI7tB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_action(state, epsilon=0):\n",
        "    \"\"\"\n",
        "    sample actions with epsilon-greedy policy\n",
        "    recap: with p = epsilon pick random action, else pick action with highest Q(s,a)\n",
        "    \"\"\"\n",
        "    \n",
        "    q_values = network.predict(state[None])[0]\n",
        "    \n",
        "    if np.random.rand()<epsilon:\n",
        "        chosen_action=np.random.choice(len(q_values))\n",
        "    else:\n",
        "        chosen_action=np.argmax(q_values)\n",
        "\n",
        "\n",
        "    return chosen_action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWJcuiNbI7tG",
        "colab_type": "code",
        "outputId": "917f6945-1168-4998-d22b-e9b25a244da9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "assert network.output_shape == (None, n_actions), \"please make sure your model maps state s -> [Q(s,a0), ..., Q(s, a_last)]\"\n",
        "assert network.layers[-1].activation == keras.activations.linear, \"please make sure you predict q-values without nonlinearity\"\n",
        "\n",
        "# test epsilon-greedy exploration\n",
        "s = env.reset()\n",
        "assert np.shape(get_action(s)) == (), \"please return just one action (integer)\"\n",
        "for eps in [0., 0.1, 0.5, 1.0]:\n",
        "    state_frequencies = np.bincount([get_action(s, epsilon=eps) for i in range(10000)], minlength=n_actions)\n",
        "    best_action = state_frequencies.argmax()\n",
        "    assert abs(state_frequencies[best_action] - 10000 * (1 - eps + eps / n_actions)) < 200\n",
        "    for other_action in range(n_actions):\n",
        "        if other_action != best_action:\n",
        "            assert abs(state_frequencies[other_action] - 10000 * (eps / n_actions)) < 200\n",
        "    print('e=%.1f tests passed'%eps)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e=0.0 tests passed\n",
            "e=0.1 tests passed\n",
            "e=0.5 tests passed\n",
            "e=1.0 tests passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDVFbbOrI7tM",
        "colab_type": "text"
      },
      "source": [
        "### Q-learning via gradient descent\n",
        "\n",
        "We shall now train our agent's Q-function by minimizing the TD loss:\n",
        "$$ L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]) ^2 $$\n",
        "\n",
        "\n",
        "Where\n",
        "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
        "* $\\gamma$ is a discount factor defined two cells above.\n",
        "\n",
        "The tricky part is with  $Q_{-}(s',a')$. From an engineering standpoint, it's the same as $Q_{\\theta}$ - the output of your neural network policy. However, when doing gradient descent, __we won't propagate gradients through it__ to make training more stable (see lectures).\n",
        "\n",
        "To do so, we shall use `tf.stop_gradient` function which basically says \"consider this thing constant when doingbackprop\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjpVt4KPI7tN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CENSORED!!!!\n",
        "# Create placeholders for the <s, a, r, s'> tuple and a special indicator for game end (is_done = True)\n",
        "\n",
        "# states_ph = keras.backend.placeholder(dtype='float32', shape=(None,) + state_dim)\n",
        "# actions_ph = keras.backend.placeholder(dtype='int32', shape=[None])\n",
        "# rewards_ph = keras.backend.placeholder(dtype='float32', shape=[None])\n",
        "# next_states_ph = keras.backend.placeholder(dtype='float32', shape=(None,) + state_dim)\n",
        "# is_done_ph = keras.backend.placeholder(dtype='bool', shape=[None])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ATrot8dI7tR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#     CENSORED!!!!\n",
        "# #get q-values for all actions in current states\n",
        "# predicted_qvalues = network(states_ph)\n",
        "\n",
        "# #select q-values for chosen actions\n",
        "# predicted_qvalues_for_actions = tf.reduce_sum(predicted_qvalues * tf.one_hot(actions_ph, n_actions), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQxmD3XpI7tX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gamma = 0.99\n",
        "\n",
        "#  CENSORED!!!!\n",
        "\n",
        "# # compute q-values for all actions in next states\n",
        "# predicted_next_qvalues = network(next_states_ph)\n",
        "\n",
        "# # compute V*(next_states) using predicted next q-values\n",
        "# next_state_values = tf.reduce_max(predicted_next_qvalues,axis=1)\n",
        "\n",
        "# # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
        "# target_qvalues_for_actions = rewards_ph+gamma*next_state_values\n",
        "\n",
        "# # at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
        "# target_qvalues_for_actions = tf.where(is_done_ph, rewards_ph, target_qvalues_for_actions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8tWv3fnI7tb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#    CENSORED!!!!\n",
        "# #mean squared error loss to minimize\n",
        "# loss = (predicted_qvalues_for_actions - tf.stop_gradient(target_qvalues_for_actions)) ** 2\n",
        "# loss = tf.reduce_mean(loss)\n",
        "\n",
        "# # training function that resembles agent.update(state, action, reward, next_state) from tabular agent\n",
        "# train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfBXofxpI7tf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#    CENSORED!!!!\n",
        "# assert tf.gradients(loss, [predicted_qvalues_for_actions])[0] is not None, \"make sure you update q-values for chosen actions and not just all actions\"\n",
        "# assert tf.gradients(loss, [predicted_next_qvalues])[0] is None, \"make sure you don't propagate gradient w.r.t. Q_(s',a')\"\n",
        "# assert predicted_next_qvalues.shape.ndims == 2, \"make sure you predicted q-values for all actions in next state\"\n",
        "# assert next_state_values.shape.ndims == 1, \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
        "# assert target_qvalues_for_actions.shape.ndims == 1, \"there's something wrong with target q-values, they must be a vector\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v19kzu-OI7tn",
        "colab_type": "text"
      },
      "source": [
        "### Playing the game (только здесь добавлял строки, в остальных местах лишь убирал)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66CHaXiKI7to",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#     CENSORED!!!!\n",
        "# sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwmK_d_wI7tt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_session(env, t_max=1000, epsilon=0, train=False):\n",
        "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
        "    total_reward = 0\n",
        "    s = env.reset()\n",
        "    \n",
        "    for t in range(t_max):\n",
        "        a = get_action(s, epsilon=epsilon)       \n",
        "        next_s, r, done, _ = env.step(a)\n",
        "        if train:\n",
        "  # CENSORED\n",
        "  #           sess.run(train_step,{\n",
        "  #               states_ph: [s], actions_ph: [a], rewards_ph: [r], \n",
        "  #               next_states_ph: [next_s], is_done_ph: [done]\n",
        "  #           })\n",
        "\n",
        "# А теперь добавляю код \n",
        "          if done:\n",
        "            target=r # Если терминальое состояние то reward и есть ценой\n",
        "          else:\n",
        "            q_next_values = network.predict(next_s.reshape(1,-1))[0]    # иначе предсказываем  Q следующего состояния для всех действий       \n",
        "            target=r+gamma*np.max(q_next_values,keepdims=True)          # и вычисляем цену текущей пары состояние-действие на основе награды и предполагаемой цены лучшего действия для следующего состояния\n",
        "\n",
        "          q_values = network.predict(s.reshape(1,-1))[0]  # получаем текущее предположение нейросети о цене действий текущего состояния\n",
        "          q_values[a]=target # подправляем одно значение Qsa на вычисленное. Как раз по нему и будет вычисляться MSE, так как Qsa остальных действий будут совпадать \n",
        "\n",
        "          network.fit(s.reshape(1,-1),q_values.reshape(1,-1),verbose=0)\n",
        "\n",
        "\n",
        "        total_reward += r\n",
        "        s = next_s\n",
        "        if done:\n",
        "            break\n",
        "            \n",
        "    return total_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu_afnggI7ty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epsilon = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz-LfTEkI7t5",
        "colab_type": "code",
        "outputId": "b6f1be0e-b4bd-4a96-c153-e93ee4a29909",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "for i in range(60):\n",
        "    session_rewards = [generate_session(env, epsilon=epsilon, train=True) for _ in range(100)]\n",
        "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), epsilon))\n",
        "    \n",
        "    epsilon *= 0.99\n",
        "    assert epsilon >= 1e-4, \"Make sure epsilon is always nonzero during training\"\n",
        "    \n",
        "    if np.mean(session_rewards) > 300:\n",
        "        print(\"You Win!\")\n",
        "        break"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch #0\tmean reward = 16.170\tepsilon = 0.500\n",
            "epoch #1\tmean reward = 13.360\tepsilon = 0.495\n",
            "epoch #2\tmean reward = 13.470\tepsilon = 0.490\n",
            "epoch #3\tmean reward = 14.190\tepsilon = 0.485\n",
            "epoch #4\tmean reward = 18.320\tepsilon = 0.480\n",
            "epoch #5\tmean reward = 21.420\tepsilon = 0.475\n",
            "epoch #6\tmean reward = 33.400\tepsilon = 0.471\n",
            "epoch #7\tmean reward = 41.620\tepsilon = 0.466\n",
            "epoch #8\tmean reward = 46.750\tepsilon = 0.461\n",
            "epoch #9\tmean reward = 62.830\tepsilon = 0.457\n",
            "epoch #10\tmean reward = 81.740\tepsilon = 0.452\n",
            "epoch #11\tmean reward = 95.050\tepsilon = 0.448\n",
            "epoch #12\tmean reward = 127.800\tepsilon = 0.443\n",
            "epoch #13\tmean reward = 125.460\tepsilon = 0.439\n",
            "epoch #14\tmean reward = 129.590\tepsilon = 0.434\n",
            "epoch #15\tmean reward = 152.900\tepsilon = 0.430\n",
            "epoch #16\tmean reward = 150.040\tepsilon = 0.426\n",
            "epoch #17\tmean reward = 208.310\tepsilon = 0.421\n",
            "epoch #18\tmean reward = 168.650\tepsilon = 0.417\n",
            "epoch #19\tmean reward = 219.500\tepsilon = 0.413\n",
            "epoch #20\tmean reward = 202.300\tepsilon = 0.409\n",
            "epoch #21\tmean reward = 224.990\tepsilon = 0.405\n",
            "epoch #22\tmean reward = 240.620\tepsilon = 0.401\n",
            "epoch #23\tmean reward = 223.910\tepsilon = 0.397\n",
            "epoch #24\tmean reward = 260.830\tepsilon = 0.393\n",
            "epoch #25\tmean reward = 345.580\tepsilon = 0.389\n",
            "You Win!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsFLSLfyVx-f",
        "colab_type": "text"
      },
      "source": [
        "Такими были результаты в Tensorflow 1.0\n",
        "\n",
        "epoch #0\tmean reward = 13.420\tepsilon = 0.500\n",
        "\n",
        "epoch #1\tmean reward = 13.520\tepsilon = 0.495\n",
        "\n",
        "epoch #2\tmean reward = 13.070\tepsilon = 0.490\n",
        "\n",
        "epoch #3\tmean reward = 13.540\tepsilon = 0.485\n",
        "\n",
        "epoch #4\tmean reward = 16.190\tepsilon = 0.480\n",
        "\n",
        "epoch #5\tmean reward = 18.860\tepsilon = 0.475\n",
        "\n",
        "epoch #6\tmean reward = 27.690\tepsilon = 0.471\n",
        "\n",
        "epoch #7\tmean reward = 38.670\tepsilon = 0.466\n",
        "\n",
        "epoch #8\tmean reward = 47.010\tepsilon = 0.461\n",
        "\n",
        "epoch #9\tmean reward = 57.540\tepsilon = 0.457\n",
        "\n",
        "epoch #10\tmean reward = 65.480\tepsilon = 0.452\n",
        "\n",
        "epoch #11\tmean reward = 97.840\tepsilon = 0.448\n",
        "\n",
        "epoch #12\tmean reward = 110.370\tepsilon = 0.443\n",
        "\n",
        "epoch #13\tmean reward = 134.370\tepsilon = 0.439\n",
        "\n",
        "epoch #14\tmean reward = 152.170\tepsilon = 0.434\n",
        "\n",
        "epoch #15\tmean reward = 155.310\tepsilon = 0.430\n",
        "\n",
        "epoch #16\tmean reward = 139.790\tepsilon = 0.426\n",
        "\n",
        "epoch #17\tmean reward = 172.470\tepsilon = 0.421\n",
        "\n",
        "epoch #18\tmean reward = 217.990\tepsilon = 0.417\n",
        "\n",
        "epoch #19\tmean reward = 267.210\tepsilon = 0.413\n",
        "\n",
        "epoch #20\tmean reward = 288.430\tepsilon = 0.409\n",
        "\n",
        "epoch #21\tmean reward = 344.230\tepsilon = 0.405\n",
        "\n",
        "Моя простая модель Q-learning в Tensorflow 2.0 работает!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LimkAZ9GI7uM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}